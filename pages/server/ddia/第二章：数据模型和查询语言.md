# 数据密集型系统设计

## 第二章：数据模型和查询语言

1.NoSQL出现的几个驱动因素：

- 需要比关系型数据库更高的可伸缩性，比如更大型的数据及更高的写吞吐。
- 普遍倾向于开源免费产品， 而不是商业数据库
- 一些特殊的查询操作，关系型数据库无法支持
- 受限于关系数据库的schema限制，希望能有更动态和易于表达的数据模型，



2.SSTable

- 首先，我们进行写入的时候，在磁盘上只能进行顺序写入，不能修改前面的值，因为修改前面的值属于随机访问，磁盘顺序访问性能更好，并且也不用担心数据写入过程中，机器宕机造成的新旧数据都只存在一部分的情况。
- 我们在内存中维护一个可以进行排序的数据结构，比如红黑树或AVL树。写入的时候，我们先写入到这个树中，等大小达到一定阈值后，就将整个树写入到磁盘中，同时可以开一个新的树进行新的写入。这样，我们就能保证写入到磁盘中的数据是排过序的。
- 对于磁盘数据，我们可以分段。后面的段的数据更新，当遇到多个段都有同一份数据的时候，使用后面的段就行了（如果是被删除，就标记一个墓碑）。在段内，我们可以进行压缩，去重。在多个段之间，我们可以进行合并，合成一个大段。
- 当我们进行查找的时候，只需要保留少量的索引数据，就可以在一个段里面快速的定位到某个数据的大概区间。从而进行磁盘寻道，即可查到数据。如下图

![img](https://cdn.jsdelivr.net/gh/oubindo/ImageBed@latest//img/spaces%252F-MHdCOHMs3fNDC20H5qi%252Fuploads%252Fgit-blob-7956675af92e8d6a5190f95d4c228fba43263572%252Ffig3-5.png)

- 查询的时候，会先查内存中有没有数据，如果没有，会从尾到头遍历所有的磁盘段。遍历磁盘段是一个耗时的操作。通常会用**布隆过滤器**来进行预先的判断。



3.B树

B树的底层写操作是用新数据复写硬盘上的旧数据，并假定覆写不改变页面的位置。

![img](https://cdn.jsdelivr.net/gh/oubindo/ImageBed@latest//img/spaces%252F-MHdCOHMs3fNDC20H5qi%252Fuploads%252Fgit-blob-48f1d22996c5f317e03c6aaf29fb5bdab8f2fbe9%252Ffig3-7.png)



## 第五章：复制

### **副本一致性**

现代的数据库系统来说，几乎都具备了复制机制`replication`，这种设计方式至少能对系统带来两个好处：

- **多副本容灾**：只要存在一个可用数据副本，数据就能够恢复
- **读性能水平扩展**：通过分布到不同的机器上，同一份数据能够在多个节点上同时供外部访问

保证多副本的数据一致性`consistency`是一个难题。

最简单的实现方式是使用同步复制机制`sync-replication`：保证写操作在所有副本上执行成功后，再响应客户端。不过这一方式通常意味着糟糕的写性能，因此较少使用。

与之相对的则是异步复制机制`async-replication`：写操作在某些副本上成功后即可响应客户端，数据库会异步将修改同步到剩余副本上。其优势在于较高的写入性能，但是数据副本的一致性无法保证。

以最基础的主从复制架构为例，虽然主库与从库最终都会到达一个一致的状态，但是主从状态同步会存在时间延迟，这类延迟被称为复制滞后`replication lag`现象。 期间可能同时存在两份相互冲突的数据副本，依赖这些数据的应用如果没有做好预防处理，最终会引发系统行为异常。

在分布式数据库中，维护副本状态的进程分为以下两类：

- **leader / master**： 能够同时处理读写请求的进程
- **follower / slave**： 只能处理读请求的进程

基于以上定义，常见的复制架构又可以分为下图中的 3 类：

![img](https://cdn.jsdelivr.net/gh/oubindo/ImageBed@latest//img/o_200430061152multi_copy_consistency.png)



| 一主一从 `master-slave` | 说明                                                         |
| ----------------------- | ------------------------------------------------------------ |
| 潜在问题                | 写后读一致性 `read-after-write consistency` 客户端修改数据，变更未同步到从库，此时从从库读取数据，得到的是未修改的结果 |
| 解决方案                | 写操作对之后的读操作立即可见，方案有：1.根据应用特点，强制部分功能只访问主库，保证读写顺序一致 2.维护用户修改时间戳与从库修改的时间戳，根据时间戳选择是否读取从库 |

| 一主多从 `single-leader` | 说明                                                         |
| ------------------------ | ------------------------------------------------------------ |
| 潜在问题                 | 客户端多次读取同一条记录，但请求路由到不同的从库上面，可能读取到旧版本的数据 |
| 解决方案                 | **单调读一致性** `monotonic read` 之后读取到的数据一定要比之前读取到的数据要新。保证同个用户的请求只会路由到同个从库，保证读命令顺序一致 |

| 多主多从 `multi-leader / leaderless` | 说明                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| 潜在问题                             | 客户端多次修改数据路由到不同的主库上，且数据之间有因果关系（例如：问答记录） 可能读取到次序混乱的数据，也可能修改一条尚未存在的数据（例如：leader 间的网络延迟） |
| 解决方案                             | **一致前缀读** `consistent prefix read` ：写操作的结果必须按照其执行的顺序被读取到具有因果关系的写操作在同个主库上执行，保证写命令顺序一致 |



### **冲突解决**

在大规模互联网应用中，多数据中心正变得越来越流行，其优点如下：

- 地理位置近，访问速度快
- 高可用，单个数据中心宕机或者网络出现问题不会导致不可用的情况

当系统需要部署到多个数据中心的时候，不可避免地会使用到 multi-leader 架构，这带来以下问题：

- 同一份数据可能被两个数据中心并发修改，导致写冲突
- 数据库的某些特性不能很好地支持多 leader 架构，例如：自增主键、触发器



#### 写入冲突

single-leader 架构下的写是顺序的，对同一份数据的修改，在每个副本上都能得到最终一致的结果。
multi-leader 架构下每个 leader 中的写也是有序的，但是不同 leader 之间的写操作是无序的，因此对同一份数据的修改也是无序的，最终导致副本的状态可能不一致。

如何保证所有库收敛到同一个状态？—— **解决冲突**

- 每个写操作关联一个唯一 ID，选取优先级最高的操作结果
- 每个副本关联一个唯一ID，选取优先级最高的副本对应的结果
- 将两个冲突的数据合并成同一条数据
- 保存所有冲突数据，由后续的操作来进行解决

解决冲突的时机又可以分为：

- 写时解决：向数据库注入解决冲突的逻辑代码，当发生冲突时由数据库进行调用`MySQL MGR 中通过维护全局一致的 Binlog 实现一致性`
- 读时解决：当存在冲突数据时，应用会获取到这些冲突数据，并自动或手动解决这些冲突`Dynamo NRW 通过调整读写副本数量来保证读取到最新的数据`













